{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SAmple: https://github.com/Piasy/Udacity-DLND/blob/master/language-translation/dlnd_language_translation.ipynb\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import numpy as np\n",
    "tf.reset_default_graph() #Clears the default graph stack and resets the global default graph.\n",
    "sess = tf.InteractiveSession() #initializes a tensorflow session\n",
    "data_file = open('cmudict.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enc: 28\n",
      "Dec: 28\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "l1=-1\n",
    "l2=-1\n",
    "for line in data_file:\n",
    "    tok = line.split()  \n",
    "    word = tok[0]\n",
    "    if '(' in word:\n",
    "        word=word[:word.index('(')]\n",
    "    if len(word) > l1:\n",
    "        l1 = len(word)\n",
    "    phones = tok[1:]\n",
    "    if len(phones) > l2:\n",
    "        l2=len(phones)\n",
    "    data.append(list([word,phones]))\n",
    "print(\"Enc:\",l1)\n",
    "print(\"Dec:\",l2)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'bout\", ['B', 'AW1', 'T']]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src__txt_to_int: {\"'\": 1, 'b': 2, 'o': 3, 'u': 4, 't': 5, 'c': 6, 'a': 7, 's': 8, 'e': 9, 'r': 10, 'm': 11, 'f': 12, 'i': 13, 'g': 14, 'n': 15, 'k': 16, 'y': 17, 'd': 18, 'l': 19, 'w': 20, '.': 21, 'h': 22, 'v': 23, 'z': 24, 'x': 25, 'j': 26, 'q': 27, '-': 28, 'p': 29, '1': 30, '<PAD>': 0, '<UNK>': 31}\n",
      "src__int_to_txt: {1: \"'\", 2: 'b', 3: 'o', 4: 'u', 5: 't', 6: 'c', 7: 'a', 8: 's', 9: 'e', 10: 'r', 11: 'm', 12: 'f', 13: 'i', 14: 'g', 15: 'n', 16: 'k', 17: 'y', 18: 'd', 19: 'l', 20: 'w', 21: '.', 22: 'h', 23: 'v', 24: 'z', 25: 'x', 26: 'j', 27: 'q', 28: '-', 29: 'p', 30: '1', 0: '<PAD>', 31: '<UNK>'}\n"
     ]
    }
   ],
   "source": [
    "f1 = open('src.txt',\"w\")\n",
    "f2 = open('tar.txt',\"w\")\n",
    "src__txt_to_int={}\n",
    "src__int_to_txt={}\n",
    "cnt=1\n",
    "for pair in data:\n",
    "    for char in pair[0]:\n",
    "        f1.write(char+\" \")\n",
    "        if char not in src__txt_to_int:\n",
    "            src__txt_to_int[char]=cnt\n",
    "            src__int_to_txt[cnt]=char\n",
    "            cnt+=1\n",
    "    f1.write(\"\\n\")\n",
    "    for phone in pair[1]:\n",
    "        f2.write(phone+\" \")\n",
    "    f2.write(\"\\n\")\n",
    "src__txt_to_int['<PAD>']=0\n",
    "src__int_to_txt[0]='<PAD>'\n",
    "src__txt_to_int['<UNK>']=cnt\n",
    "src__int_to_txt[cnt]='<UNK>'\n",
    "print(\"src__txt_to_int:\",src__txt_to_int)\n",
    "print(\"src__int_to_txt:\",src__int_to_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt->int: {'AA': 1, 'AA0': 2, 'AA1': 3, 'AA2': 4, 'AE': 5, 'AE0': 6, 'AE1': 7, 'AE2': 8, 'AH': 9, 'AH0': 10, 'AH1': 11, 'AH2': 12, 'AO': 13, 'AO0': 14, 'AO1': 15, 'AO2': 16, 'AW': 17, 'AW0': 18, 'AW1': 19, 'AW2': 20, 'AY': 21, 'AY0': 22, 'AY1': 23, 'AY2': 24, 'B': 25, 'CH': 26, 'D': 27, 'DH': 28, 'EH': 29, 'EH0': 30, 'EH1': 31, 'EH2': 32, 'ER': 33, 'ER0': 34, 'ER1': 35, 'ER2': 36, 'EY': 37, 'EY0': 38, 'EY1': 39, 'EY2': 40, 'F': 41, 'G': 42, 'HH': 43, 'IH': 44, 'IH0': 45, 'IH1': 46, 'IH2': 47, 'IY': 48, 'IY0': 49, 'IY1': 50, 'IY2': 51, 'JH': 52, 'K': 53, 'L': 54, 'M': 55, 'N': 56, 'NG': 57, 'OW': 58, 'OW0': 59, 'OW1': 60, 'OW2': 61, 'OY': 62, 'OY0': 63, 'OY1': 64, 'OY2': 65, 'P': 66, 'R': 67, 'S': 68, 'SH': 69, 'T': 70, 'TH': 71, 'UH': 72, 'UH0': 73, 'UH1': 74, 'UH2': 75, 'UW': 76, 'UW0': 77, 'UW1': 78, 'UW2': 79, 'V': 80, 'W': 81, 'Y': 82, 'Z': 83, 'ZH': 84, '<EOS>': 85, '<GO>': 86, '<UNK>': 87, '<PAD>': 0}\n",
      "int->txt: {1: 'AA', 2: 'AA0', 3: 'AA1', 4: 'AA2', 5: 'AE', 6: 'AE0', 7: 'AE1', 8: 'AE2', 9: 'AH', 10: 'AH0', 11: 'AH1', 12: 'AH2', 13: 'AO', 14: 'AO0', 15: 'AO1', 16: 'AO2', 17: 'AW', 18: 'AW0', 19: 'AW1', 20: 'AW2', 21: 'AY', 22: 'AY0', 23: 'AY1', 24: 'AY2', 25: 'B', 26: 'CH', 27: 'D', 28: 'DH', 29: 'EH', 30: 'EH0', 31: 'EH1', 32: 'EH2', 33: 'ER', 34: 'ER0', 35: 'ER1', 36: 'ER2', 37: 'EY', 38: 'EY0', 39: 'EY1', 40: 'EY2', 41: 'F', 42: 'G', 43: 'HH', 44: 'IH', 45: 'IH0', 46: 'IH1', 47: 'IH2', 48: 'IY', 49: 'IY0', 50: 'IY1', 51: 'IY2', 52: 'JH', 53: 'K', 54: 'L', 55: 'M', 56: 'N', 57: 'NG', 58: 'OW', 59: 'OW0', 60: 'OW1', 61: 'OW2', 62: 'OY', 63: 'OY0', 64: 'OY1', 65: 'OY2', 66: 'P', 67: 'R', 68: 'S', 69: 'SH', 70: 'T', 71: 'TH', 72: 'UH', 73: 'UH0', 74: 'UH1', 75: 'UH2', 76: 'UW', 77: 'UW0', 78: 'UW1', 79: 'UW2', 80: 'V', 81: 'W', 82: 'Y', 83: 'Z', 84: 'ZH', 85: '<EOS>', 86: '<GO>', 87: '<UNK>', 0: '<PAD>'}\n"
     ]
    }
   ],
   "source": [
    "phonef = open('cmudict.symbols','r')\n",
    "tar__txt_to_int={}\n",
    "tar__int_to_txt={}\n",
    "cnt=1\n",
    "for phone in phonef:\n",
    "    phone=phone.split()\n",
    "    tar__txt_to_int[phone[0]]=cnt\n",
    "    tar__int_to_txt[cnt]=phone[0]\n",
    "    cnt+=1\n",
    "tar__txt_to_int['<EOS>']=cnt\n",
    "tar__int_to_txt[cnt]='<EOS>'\n",
    "cnt+=1\n",
    "tar__txt_to_int['<GO>']=cnt\n",
    "tar__int_to_txt[cnt]='<GO>'\n",
    "cnt+=1\n",
    "tar__txt_to_int['<UNK>']=cnt\n",
    "tar__int_to_txt[cnt]='<UNK>'\n",
    "tar__txt_to_int['<PAD>']=0\n",
    "tar__int_to_txt[0]='<PAD>'\n",
    "phonef.close()\n",
    "print(\"txt->int:\",tar__txt_to_int)\n",
    "print(\"int->txt:\",tar__int_to_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write new files in a way that tf can understand...\n",
    "f1ints = open('int_src.txt','w')\n",
    "f1 = open('src.txt',\"r\")\n",
    "src_f=[]\n",
    "tar_f=[]\n",
    "for line in f1:\n",
    "    temp=[]\n",
    "    toks = line.split()\n",
    "    for tok in toks:\n",
    "        f1ints.write(str(src__txt_to_int[tok])+\" \")\n",
    "        temp.append(src__txt_to_int[tok])\n",
    "    src_f.append(temp)\n",
    "    f1ints.write(\"\\n\")\n",
    "f2ints = open('int_tar.txt','w')\n",
    "\n",
    "f2 = open('tar.txt',\"r\")\n",
    "for line in f2:\n",
    "    temp=[]\n",
    "    toks = line.split()\n",
    "    for tok in toks:\n",
    "        if tok == '#':\n",
    "            break\n",
    "        f2ints.write(str(tar__txt_to_int[tok])+\" \")\n",
    "        temp.append(tar__txt_to_int[tok])\n",
    "    temp.append(tar__txt_to_int['<EOS>'])\n",
    "    f2ints.write(str(tar__txt_to_int['<EOS>'])+'\\n')\n",
    "    tar_f.append(temp)\n",
    "f1.close()\n",
    "f2.close()\n",
    "f1ints.close()\n",
    "f2ints.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5], [1, 6, 7, 4, 8, 9], [1, 6, 3, 4, 10, 8, 9], [1, 6, 4, 8, 9], [1, 9, 11], [1, 12, 10, 13, 8, 6, 3], [1, 14, 7, 13, 15], [1, 16, 7, 17], [1, 11], [1, 15]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preprocess finished...\n",
    "\n",
    "Split cmudict.dict into source sequence (words are letters in this sequence(makes sense!?))\n",
    "and target sequences (these are the phones) in two files:\n",
    "src sequences: src.txt\n",
    "tar sequences: tar.txt\n",
    "'''\n",
    "src_path='src.txt'\n",
    "tar_path='tar.txt'\n",
    "\n",
    "'''\n",
    "\n",
    "!!!IMPORTANT INFORMATION!!!\n",
    "datapoints with multi prounciations: src1 sequence = scr2 sequence but matching tar1 sequence != tar2 sequence\n",
    "How does this affect learning?\n",
    "\n",
    "datapoints with forgin pronunications simmilar\n",
    "How does this affect learning?\n",
    "\n",
    "should we just remove this points altogether?\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "Source:\n",
    "text_to_int dict: src__txt_to_int\n",
    "int_to_text dict: src__int_to_txt\n",
    "\n",
    "Target:\n",
    "text_to_int dict: tar__txt_to_int\n",
    "int_to_tect dict: tar__int_to_txt\n",
    "\n",
    "\n",
    "Used dictionaries to map sequences in a way that tf can understand:\n",
    "\n",
    "Source_id'd: int_src.txt\n",
    "Target_id'd: int_tar.txt\n",
    "'''\n",
    "print(src_f[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now lets save this in a pickle file...\n",
    "'''\n",
    "#first get id's in a list:\n",
    "src_ids=[]\n",
    "tar_ids=[]\n",
    "f1 = open('int_src.txt','r')\n",
    "f2 = open('int_tar.txt','r')\n",
    "for line in f1:\n",
    "    toks = line.split()\n",
    "    src_ids.append(toks)\n",
    "for line in f2:\n",
    "    toks = line.split()\n",
    "    tar_ids.append(toks)\n",
    "f1.close()\n",
    "f2.close()\n",
    "import pickle\n",
    "with open('preprocess.p', 'wb') as out_file:\n",
    "    pickle.dump((\n",
    "            (src_f, tar_f),\n",
    "            (src__txt_to_int, tar__txt_to_int),\n",
    "            (src__int_to_txt, tar__int_to_txt)), out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Sample XXXX\n",
    "def model_inputs():\n",
    "    \n",
    "    \"\"\"\n",
    "    Create TF Placeholders for:\n",
    "    input, \n",
    "    targets, \n",
    "    learning rate, \n",
    "    lengths of source \n",
    "    target sequences\n",
    "    return Tuple (input, targets, learning rate, keep probability, target sequence length,\n",
    "    max target sequence length, source sequence length)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    inputs = tf.placeholder(tf.int32,[None,None],name=\"input\")\n",
    "    targets = tf.placeholder(tf.int32,[None,None])\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "    target_seq = tf.placeholder(tf.int32,[None],name=\"target_sequence_length\")\n",
    "    max_target = tf.reduce_max(target_seq,name=\"max_target_len\")                               \n",
    "    source_seq = tf.placeholder(tf.int32,[None],name=\"source_sequence_length\")\n",
    "    return (inputs, targets, learning_rate, keep_prob, target_seq, max_target, source_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Sample XXXX\n",
    "def process_decoder_input(target_data, tar__txt_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    :param target_data: Target Placehoder\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    cut_off = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], tar__txt_to_int['<GO>']), cut_off], 1)\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Sample XXXX\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create encoding layer\n",
    "    :param rnn_inputs: Inputs for the RNN\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :param source_sequence_length: a list of the lengths of each sequence in the batch\n",
    "    :param source_vocab_size: vocabulary size of source data\n",
    "    :param encoding_embedding_size: embedding size of source data\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    # Encoder embedding\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(rnn_inputs, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "    # RNN cell\n",
    "    def make_cell(rnn_size,keep_prob):\n",
    "        enc_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(enc_cell, output_keep_prob = keep_prob)\n",
    "        return drop\n",
    "\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size,keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Sample XXXX\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    :param encoder_state: Encoder State\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param target_sequence_length: The lengths of each sequence in the target batch\n",
    "    :param max_summary_length: The length of the longest sequence in the batch\n",
    "    :param output_layer: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=target_sequence_length,\n",
    "                                                            time_major=False)\n",
    "    \n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           encoder_state,\n",
    "                                                           output_layer) \n",
    "    \n",
    "    BasicDecoderOutput = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                       impute_finished=True,\n",
    "                                                                       maximum_iterations=max_summary_length)[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    return BasicDecoderOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Sample XXXX\n",
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for inference\n",
    "    :param encoder_state: Encoder state\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param start_of_sequence_id: GO ID\n",
    "    :param end_of_sequence_id: EOS Id\n",
    "    :param max_target_sequence_length: Maximum length of target sequences\n",
    "    :param vocab_size: Size of decoder/target vocabulary\n",
    "    :param decoding_scope: TenorFlow Variable Scope for decoding\n",
    "    :param output_layer: Function to apply the output layer\n",
    "    :param batch_size: Batch size\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([start_of_sequence_id], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_of_sequence_id)\n",
    "\n",
    "    # Basic decoder\n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                    inference_helper,\n",
    "                                                    encoder_state,\n",
    "                                                    output_layer)\n",
    "\n",
    "    # Perform dynamic decoding using the decoder\n",
    "    BasicDecoderOutput = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                        impute_finished=True,\n",
    "                                                        maximum_iterations=max_target_sequence_length)[0]\n",
    "    return BasicDecoderOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Sample XXXX\n",
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :param dec_input: Decoder input\n",
    "    :param encoder_state: Encoder state\n",
    "    :param target_sequence_length: The lengths of each sequence in the target batch\n",
    "    :param max_target_sequence_length: Maximum length of target sequences\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param target_vocab_size: Size of target vocabulary\n",
    "    :param batch_size: The size of the batch\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :param decoding_embedding_size: Decoding embedding size\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    # 2. Construct the decoder cell\n",
    "    def make_cell(rnn_size):\n",
    "        dec_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return dec_cell\n",
    "\n",
    "    rnnCell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    output_layer = Dense(target_vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    Training_BasicDecoderOutput = decoding_layer_train(encoder_state, rnnCell, dec_embed_input, target_sequence_length, max_target_sequence_length, output_layer, keep_prob)\n",
    "    \n",
    "    Inference_BasicDecoderOutput = decoding_layer_infer(encoder_state, rnnCell, dec_embeddings, target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], max_target_sequence_length, target_vocab_size, output_layer, batch_size, keep_prob)\n",
    "    \n",
    "    return Training_BasicDecoderOutput, Inference_BasicDecoderOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Sample XXXX\n",
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  source_sequence_length, target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence part of the neural network\n",
    "    :param input_data: Input placeholder\n",
    "    :param target_data: Target placeholder\n",
    "    :param keep_prob: Dropout keep probability placeholder\n",
    "    :param batch_size: Batch Size\n",
    "    :param source_sequence_length: Sequence Lengths of source sequences in the batch\n",
    "    :param target_sequence_length: Sequence Lengths of target sequences in the batch\n",
    "    :param source_vocab_size: Source vocabulary size\n",
    "    :param target_vocab_size: Target vocabulary size\n",
    "    :param enc_embedding_size: Decoder embedding size\n",
    "    :param dec_embedding_size: Encoder embedding size\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    _, enc_state= encoding_layer(input_data, rnn_size, num_layers, keep_prob,  source_sequence_length, source_vocab_size, enc_embedding_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(target_data, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    Training_BasicDecoderOutput, Inference_BasicDecoderOutput = decoding_layer(dec_input, enc_state, target_sequence_length, max_target_sentence_length, rnn_size, num_layers, target_vocab_to_int, target_vocab_size, batch_size, keep_prob, dec_embedding_size)\n",
    "    \n",
    "    return Training_BasicDecoderOutput, Inference_BasicDecoderOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 5\n",
    "# Batch Size\n",
    "batch_size = 64\n",
    "# RNN Size\n",
    "rnn_size = 128\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 30\n",
    "decoding_embedding_size = 30\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.5\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Sample XXXX\n",
    "save_path = 'checkpoints/dev'\n",
    "with open('preprocess.p', mode='rb') as in_file:\n",
    "    LOADED = pickle.load(in_file)\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = LOADED\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob, target_sequence_length, max_target_sequence_length, source_sequence_length = model_inputs()\n",
    "\n",
    "    #sequence_length = tf.placeholder_with_default(max_target_sentence_length, None, name='sequence_length')\n",
    "    input_shape = tf.shape(input_data)\n",
    "\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   source_sequence_length,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "\n",
    "\n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Sample XXXX\n",
    "\n",
    "#XXXXXX MODIFIED FOR OUT PROBLEM XXXXXXX#\n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad char seq with <PAD> so that each char sequence of a batch has the same length\"\"\"\n",
    "    max_seq = max([len(seq) for seq in sentence_batch])\n",
    "    padded_batch=[]\n",
    "    for seq in sentence_batch:\n",
    "        temp=[]\n",
    "        for char in seq:\n",
    "            temp.append(char)\n",
    "        for x in range(len(temp),max_seq):\n",
    "            temp.append(pad_int)\n",
    "        padded_batch.append(temp)\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    \n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        \n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch   10/2109 - Train Accuracy: 0.5012, Validation Accuracy: 0.4184, Loss: 2.3969\n",
      "Epoch   0 Batch   20/2109 - Train Accuracy: 0.3164, Validation Accuracy: 0.4306, Loss: 2.8214\n",
      "Epoch   0 Batch   30/2109 - Train Accuracy: 0.4549, Validation Accuracy: 0.4670, Loss: 2.3230\n",
      "Epoch   0 Batch   40/2109 - Train Accuracy: 0.4362, Validation Accuracy: 0.4410, Loss: 2.4410\n",
      "Epoch   0 Batch   50/2109 - Train Accuracy: 0.5917, Validation Accuracy: 0.3490, Loss: 1.7906\n",
      "Epoch   0 Batch   60/2109 - Train Accuracy: 0.3828, Validation Accuracy: 0.4792, Loss: 2.3684\n",
      "Epoch   0 Batch   70/2109 - Train Accuracy: 0.4111, Validation Accuracy: 0.4688, Loss: 2.5031\n",
      "Epoch   0 Batch   80/2109 - Train Accuracy: 0.3203, Validation Accuracy: 0.4705, Loss: 2.5796\n",
      "Epoch   0 Batch   90/2109 - Train Accuracy: 0.4281, Validation Accuracy: 0.4792, Loss: 2.3492\n",
      "Epoch   0 Batch  100/2109 - Train Accuracy: 0.5024, Validation Accuracy: 0.4809, Loss: 1.8822\n",
      "Epoch   0 Batch  110/2109 - Train Accuracy: 0.2951, Validation Accuracy: 0.4774, Loss: 2.8301\n",
      "Epoch   0 Batch  120/2109 - Train Accuracy: 0.4753, Validation Accuracy: 0.4774, Loss: 1.9369\n",
      "Epoch   0 Batch  130/2109 - Train Accuracy: 0.4591, Validation Accuracy: 0.4635, Loss: 2.0228\n",
      "Epoch   0 Batch  140/2109 - Train Accuracy: 0.4901, Validation Accuracy: 0.4705, Loss: 1.8764\n",
      "Epoch   0 Batch  150/2109 - Train Accuracy: 0.5716, Validation Accuracy: 0.4826, Loss: 1.7442\n",
      "Epoch   0 Batch  160/2109 - Train Accuracy: 0.4469, Validation Accuracy: 0.4948, Loss: 1.9608\n",
      "Epoch   0 Batch  170/2109 - Train Accuracy: 0.4957, Validation Accuracy: 0.4688, Loss: 1.8487\n",
      "Epoch   0 Batch  180/2109 - Train Accuracy: 0.5893, Validation Accuracy: 0.4462, Loss: 1.4638\n",
      "Epoch   0 Batch  190/2109 - Train Accuracy: 0.4724, Validation Accuracy: 0.4688, Loss: 1.8471\n",
      "Epoch   0 Batch  200/2109 - Train Accuracy: 0.4969, Validation Accuracy: 0.4462, Loss: 1.8306\n",
      "Epoch   0 Batch  210/2109 - Train Accuracy: 0.4781, Validation Accuracy: 0.4653, Loss: 2.1655\n",
      "Epoch   0 Batch  220/2109 - Train Accuracy: 0.5365, Validation Accuracy: 0.4757, Loss: 1.5810\n",
      "Epoch   0 Batch  230/2109 - Train Accuracy: 0.5052, Validation Accuracy: 0.4740, Loss: 1.8122\n",
      "Epoch   0 Batch  240/2109 - Train Accuracy: 0.5540, Validation Accuracy: 0.4531, Loss: 1.5781\n",
      "Epoch   0 Batch  250/2109 - Train Accuracy: 0.4943, Validation Accuracy: 0.4462, Loss: 1.9450\n",
      "Epoch   0 Batch  260/2109 - Train Accuracy: 0.5453, Validation Accuracy: 0.5139, Loss: 1.7310\n",
      "Epoch   0 Batch  270/2109 - Train Accuracy: 0.4583, Validation Accuracy: 0.5174, Loss: 2.2073\n",
      "Epoch   0 Batch  280/2109 - Train Accuracy: 0.4453, Validation Accuracy: 0.4931, Loss: 1.9261\n",
      "Epoch   0 Batch  290/2109 - Train Accuracy: 0.3594, Validation Accuracy: 0.4896, Loss: 2.1417\n",
      "Epoch   0 Batch  300/2109 - Train Accuracy: 0.4431, Validation Accuracy: 0.4861, Loss: 1.8729\n",
      "Epoch   0 Batch  310/2109 - Train Accuracy: 0.4531, Validation Accuracy: 0.5295, Loss: 2.0096\n",
      "Epoch   0 Batch  320/2109 - Train Accuracy: 0.5028, Validation Accuracy: 0.4931, Loss: 1.7279\n",
      "Epoch   0 Batch  330/2109 - Train Accuracy: 0.6389, Validation Accuracy: 0.4965, Loss: 1.3161\n",
      "Epoch   0 Batch  340/2109 - Train Accuracy: 0.4002, Validation Accuracy: 0.4757, Loss: 1.9263\n",
      "Epoch   0 Batch  350/2109 - Train Accuracy: 0.4469, Validation Accuracy: 0.4722, Loss: 2.0352\n",
      "Epoch   0 Batch  360/2109 - Train Accuracy: 0.6138, Validation Accuracy: 0.4861, Loss: 1.5802\n",
      "Epoch   0 Batch  370/2109 - Train Accuracy: 0.4807, Validation Accuracy: 0.4913, Loss: 1.7858\n",
      "Epoch   0 Batch  380/2109 - Train Accuracy: 0.4458, Validation Accuracy: 0.5122, Loss: 2.0931\n",
      "Epoch   0 Batch  390/2109 - Train Accuracy: 0.5879, Validation Accuracy: 0.4896, Loss: 1.6165\n",
      "Epoch   0 Batch  400/2109 - Train Accuracy: 0.5413, Validation Accuracy: 0.4861, Loss: 1.5801\n",
      "Epoch   0 Batch  410/2109 - Train Accuracy: 0.5052, Validation Accuracy: 0.4722, Loss: 1.6857\n",
      "Epoch   0 Batch  420/2109 - Train Accuracy: 0.5495, Validation Accuracy: 0.4132, Loss: 1.6149\n",
      "Epoch   0 Batch  430/2109 - Train Accuracy: 0.5651, Validation Accuracy: 0.5069, Loss: 1.7029\n",
      "Epoch   0 Batch  440/2109 - Train Accuracy: 0.4517, Validation Accuracy: 0.5052, Loss: 2.1483\n",
      "Epoch   0 Batch  450/2109 - Train Accuracy: 0.4560, Validation Accuracy: 0.5312, Loss: 1.8268\n",
      "Epoch   0 Batch  460/2109 - Train Accuracy: 0.6797, Validation Accuracy: 0.5191, Loss: 1.3034\n",
      "Epoch   0 Batch  470/2109 - Train Accuracy: 0.4119, Validation Accuracy: 0.4896, Loss: 1.9773\n",
      "Epoch   0 Batch  480/2109 - Train Accuracy: 0.4327, Validation Accuracy: 0.4965, Loss: 1.9983\n",
      "Epoch   0 Batch  490/2109 - Train Accuracy: 0.5365, Validation Accuracy: 0.4861, Loss: 1.6635\n",
      "Epoch   0 Batch  500/2109 - Train Accuracy: 0.4596, Validation Accuracy: 0.4566, Loss: 2.0493\n",
      "Epoch   0 Batch  510/2109 - Train Accuracy: 0.4982, Validation Accuracy: 0.4861, Loss: 1.9099\n",
      "Epoch   0 Batch  520/2109 - Train Accuracy: 0.5667, Validation Accuracy: 0.4618, Loss: 1.6079\n",
      "Epoch   0 Batch  530/2109 - Train Accuracy: 0.4703, Validation Accuracy: 0.4948, Loss: 1.7200\n",
      "Epoch   0 Batch  540/2109 - Train Accuracy: 0.5085, Validation Accuracy: 0.4705, Loss: 1.5627\n",
      "Epoch   0 Batch  550/2109 - Train Accuracy: 0.5219, Validation Accuracy: 0.4549, Loss: 1.6174\n",
      "Epoch   0 Batch  560/2109 - Train Accuracy: 0.3351, Validation Accuracy: 0.5000, Loss: 2.7636\n",
      "Epoch   0 Batch  570/2109 - Train Accuracy: 0.6055, Validation Accuracy: 0.4948, Loss: 1.3892\n",
      "Epoch   0 Batch  580/2109 - Train Accuracy: 0.5599, Validation Accuracy: 0.4792, Loss: 1.6342\n",
      "Epoch   0 Batch  590/2109 - Train Accuracy: 0.3762, Validation Accuracy: 0.4826, Loss: 2.0204\n",
      "Epoch   0 Batch  600/2109 - Train Accuracy: 0.4900, Validation Accuracy: 0.4444, Loss: 1.9051\n",
      "Epoch   0 Batch  610/2109 - Train Accuracy: 0.4805, Validation Accuracy: 0.5000, Loss: 1.7474\n",
      "Epoch   0 Batch  620/2109 - Train Accuracy: 0.4779, Validation Accuracy: 0.5347, Loss: 1.8603\n",
      "Epoch   0 Batch  630/2109 - Train Accuracy: 0.4562, Validation Accuracy: 0.4115, Loss: 1.9092\n",
      "Epoch   0 Batch  640/2109 - Train Accuracy: 0.4323, Validation Accuracy: 0.4861, Loss: 1.9206\n",
      "Epoch   0 Batch  650/2109 - Train Accuracy: 0.4875, Validation Accuracy: 0.5122, Loss: 1.6967\n",
      "Epoch   0 Batch  660/2109 - Train Accuracy: 0.5156, Validation Accuracy: 0.4132, Loss: 1.5187\n",
      "Epoch   0 Batch  670/2109 - Train Accuracy: 0.5143, Validation Accuracy: 0.4514, Loss: 1.6025\n",
      "Epoch   0 Batch  680/2109 - Train Accuracy: 0.5036, Validation Accuracy: 0.4861, Loss: 1.6130\n",
      "Epoch   0 Batch  690/2109 - Train Accuracy: 0.4844, Validation Accuracy: 0.4375, Loss: 1.6500\n",
      "Epoch   0 Batch  700/2109 - Train Accuracy: 0.5185, Validation Accuracy: 0.4635, Loss: 1.8344\n",
      "Epoch   0 Batch  710/2109 - Train Accuracy: 0.4602, Validation Accuracy: 0.4115, Loss: 1.9893\n",
      "Epoch   0 Batch  720/2109 - Train Accuracy: 0.3781, Validation Accuracy: 0.4462, Loss: 2.1520\n",
      "Epoch   0 Batch  730/2109 - Train Accuracy: 0.2695, Validation Accuracy: 0.4306, Loss: 2.0989\n",
      "Epoch   0 Batch  740/2109 - Train Accuracy: 0.3763, Validation Accuracy: 0.4340, Loss: 2.0939\n",
      "Epoch   0 Batch  750/2109 - Train Accuracy: 0.5117, Validation Accuracy: 0.5174, Loss: 1.5233\n",
      "Epoch   0 Batch  760/2109 - Train Accuracy: 0.4858, Validation Accuracy: 0.4253, Loss: 1.6292\n",
      "Epoch   0 Batch  770/2109 - Train Accuracy: 0.4453, Validation Accuracy: 0.4722, Loss: 1.5699\n",
      "Epoch   0 Batch  780/2109 - Train Accuracy: 0.6310, Validation Accuracy: 0.4983, Loss: 1.3164\n",
      "Epoch   0 Batch  790/2109 - Train Accuracy: 0.5125, Validation Accuracy: 0.4167, Loss: 1.5132\n",
      "Epoch   0 Batch  800/2109 - Train Accuracy: 0.4972, Validation Accuracy: 0.4288, Loss: 1.5985\n",
      "Epoch   0 Batch  810/2109 - Train Accuracy: 0.4094, Validation Accuracy: 0.4149, Loss: 2.1835\n",
      "Epoch   0 Batch  820/2109 - Train Accuracy: 0.5909, Validation Accuracy: 0.4288, Loss: 1.4575\n",
      "Epoch   0 Batch  830/2109 - Train Accuracy: 0.5578, Validation Accuracy: 0.4132, Loss: 1.3279\n",
      "Epoch   0 Batch  840/2109 - Train Accuracy: 0.4830, Validation Accuracy: 0.4514, Loss: 1.5641\n",
      "Epoch   0 Batch  850/2109 - Train Accuracy: 0.5078, Validation Accuracy: 0.4167, Loss: 1.5186\n",
      "Epoch   0 Batch  860/2109 - Train Accuracy: 0.5848, Validation Accuracy: 0.4635, Loss: 1.3110\n",
      "Epoch   0 Batch  870/2109 - Train Accuracy: 0.5031, Validation Accuracy: 0.4184, Loss: 1.4003\n",
      "Epoch   0 Batch  880/2109 - Train Accuracy: 0.4261, Validation Accuracy: 0.3906, Loss: 1.8646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  890/2109 - Train Accuracy: 0.6031, Validation Accuracy: 0.4288, Loss: 1.4312\n",
      "Epoch   0 Batch  900/2109 - Train Accuracy: 0.4792, Validation Accuracy: 0.4132, Loss: 1.5415\n",
      "Epoch   0 Batch  910/2109 - Train Accuracy: 0.4730, Validation Accuracy: 0.4462, Loss: 2.0431\n",
      "Epoch   0 Batch  920/2109 - Train Accuracy: 0.3527, Validation Accuracy: 0.4410, Loss: 2.0073\n",
      "Epoch   0 Batch  930/2109 - Train Accuracy: 0.4427, Validation Accuracy: 0.4392, Loss: 1.8213\n",
      "Epoch   0 Batch  940/2109 - Train Accuracy: 0.5117, Validation Accuracy: 0.4184, Loss: 1.5637\n",
      "Epoch   0 Batch  950/2109 - Train Accuracy: 0.4875, Validation Accuracy: 0.4097, Loss: 1.5905\n",
      "Epoch   0 Batch  960/2109 - Train Accuracy: 0.5703, Validation Accuracy: 0.5035, Loss: 1.4505\n",
      "Epoch   0 Batch  970/2109 - Train Accuracy: 0.4883, Validation Accuracy: 0.5139, Loss: 1.3915\n",
      "Epoch   0 Batch  980/2109 - Train Accuracy: 0.6239, Validation Accuracy: 0.4462, Loss: 1.2446\n",
      "Epoch   0 Batch  990/2109 - Train Accuracy: 0.4531, Validation Accuracy: 0.4045, Loss: 1.9314\n",
      "Epoch   0 Batch 1000/2109 - Train Accuracy: 0.5582, Validation Accuracy: 0.4705, Loss: 1.2638\n",
      "Epoch   0 Batch 1010/2109 - Train Accuracy: 0.5597, Validation Accuracy: 0.4271, Loss: 1.4495\n",
      "Epoch   0 Batch 1020/2109 - Train Accuracy: 0.4288, Validation Accuracy: 0.4236, Loss: 1.7136\n",
      "Epoch   0 Batch 1030/2109 - Train Accuracy: 0.5597, Validation Accuracy: 0.4653, Loss: 1.3586\n",
      "Epoch   0 Batch 1040/2109 - Train Accuracy: 0.5142, Validation Accuracy: 0.4375, Loss: 1.4632\n",
      "Epoch   0 Batch 1050/2109 - Train Accuracy: 0.5341, Validation Accuracy: 0.4323, Loss: 1.4277\n",
      "Epoch   0 Batch 1060/2109 - Train Accuracy: 0.4460, Validation Accuracy: 0.4826, Loss: 1.5758\n",
      "Epoch   0 Batch 1070/2109 - Train Accuracy: 0.5719, Validation Accuracy: 0.4601, Loss: 1.4360\n",
      "Epoch   0 Batch 1080/2109 - Train Accuracy: 0.4462, Validation Accuracy: 0.4705, Loss: 1.7133\n",
      "Epoch   0 Batch 1090/2109 - Train Accuracy: 0.4906, Validation Accuracy: 0.4757, Loss: 1.6372\n",
      "Epoch   0 Batch 1100/2109 - Train Accuracy: 0.5417, Validation Accuracy: 0.4757, Loss: 1.4429\n",
      "Epoch   0 Batch 1110/2109 - Train Accuracy: 0.5299, Validation Accuracy: 0.4132, Loss: 1.3159\n",
      "Epoch   0 Batch 1120/2109 - Train Accuracy: 0.5649, Validation Accuracy: 0.4757, Loss: 1.4760\n",
      "Epoch   0 Batch 1130/2109 - Train Accuracy: 0.5404, Validation Accuracy: 0.4792, Loss: 1.3686\n",
      "Epoch   0 Batch 1140/2109 - Train Accuracy: 0.3750, Validation Accuracy: 0.4774, Loss: 2.2168\n",
      "Epoch   0 Batch 1150/2109 - Train Accuracy: 0.4714, Validation Accuracy: 0.4688, Loss: 1.6278\n",
      "Epoch   0 Batch 1160/2109 - Train Accuracy: 0.5094, Validation Accuracy: 0.4601, Loss: 1.4202\n",
      "Epoch   0 Batch 1170/2109 - Train Accuracy: 0.4344, Validation Accuracy: 0.4722, Loss: 1.5641\n",
      "Epoch   0 Batch 1180/2109 - Train Accuracy: 0.6194, Validation Accuracy: 0.4566, Loss: 1.2552\n",
      "Epoch   0 Batch 1190/2109 - Train Accuracy: 0.3835, Validation Accuracy: 0.4601, Loss: 1.6975\n",
      "Epoch   0 Batch 1200/2109 - Train Accuracy: 0.4787, Validation Accuracy: 0.4167, Loss: 1.5259\n",
      "Epoch   0 Batch 1210/2109 - Train Accuracy: 0.5977, Validation Accuracy: 0.4757, Loss: 1.3203\n",
      "Epoch   0 Batch 1220/2109 - Train Accuracy: 0.5971, Validation Accuracy: 0.4583, Loss: 1.2723\n",
      "Epoch   0 Batch 1230/2109 - Train Accuracy: 0.4961, Validation Accuracy: 0.4462, Loss: 1.5301\n",
      "Epoch   0 Batch 1240/2109 - Train Accuracy: 0.5554, Validation Accuracy: 0.4184, Loss: 1.3159\n",
      "Epoch   0 Batch 1250/2109 - Train Accuracy: 0.4594, Validation Accuracy: 0.4549, Loss: 1.7440\n",
      "Epoch   0 Batch 1260/2109 - Train Accuracy: 0.5335, Validation Accuracy: 0.4549, Loss: 1.4887\n",
      "Epoch   0 Batch 1270/2109 - Train Accuracy: 0.5284, Validation Accuracy: 0.4462, Loss: 1.2976\n",
      "Epoch   0 Batch 1280/2109 - Train Accuracy: 0.5271, Validation Accuracy: 0.4080, Loss: 1.6720\n",
      "Epoch   0 Batch 1290/2109 - Train Accuracy: 0.4203, Validation Accuracy: 0.4444, Loss: 2.1143\n",
      "Epoch   0 Batch 1300/2109 - Train Accuracy: 0.5028, Validation Accuracy: 0.4618, Loss: 1.4959\n",
      "Epoch   0 Batch 1310/2109 - Train Accuracy: 0.4857, Validation Accuracy: 0.4340, Loss: 1.5612\n",
      "Epoch   0 Batch 1320/2109 - Train Accuracy: 0.5440, Validation Accuracy: 0.4375, Loss: 1.3819\n",
      "Epoch   0 Batch 1330/2109 - Train Accuracy: 0.4010, Validation Accuracy: 0.4462, Loss: 1.9135\n",
      "Epoch   0 Batch 1340/2109 - Train Accuracy: 0.3420, Validation Accuracy: 0.4253, Loss: 2.5498\n",
      "Epoch   0 Batch 1350/2109 - Train Accuracy: 0.4062, Validation Accuracy: 0.4688, Loss: 2.0917\n",
      "Epoch   0 Batch 1360/2109 - Train Accuracy: 0.4542, Validation Accuracy: 0.4549, Loss: 1.6699\n",
      "Epoch   0 Batch 1370/2109 - Train Accuracy: 0.5201, Validation Accuracy: 0.4757, Loss: 1.4069\n",
      "Epoch   0 Batch 1380/2109 - Train Accuracy: 0.3694, Validation Accuracy: 0.4757, Loss: 1.8089\n",
      "Epoch   0 Batch 1390/2109 - Train Accuracy: 0.5137, Validation Accuracy: 0.4618, Loss: 1.4042\n",
      "Epoch   0 Batch 1400/2109 - Train Accuracy: 0.6752, Validation Accuracy: 0.4774, Loss: 1.0511\n",
      "Epoch   0 Batch 1410/2109 - Train Accuracy: 0.5716, Validation Accuracy: 0.4358, Loss: 1.2068\n",
      "Epoch   0 Batch 1420/2109 - Train Accuracy: 0.5482, Validation Accuracy: 0.4479, Loss: 1.3334\n",
      "Epoch   0 Batch 1430/2109 - Train Accuracy: 0.5170, Validation Accuracy: 0.4635, Loss: 1.4459\n",
      "Epoch   0 Batch 1440/2109 - Train Accuracy: 0.5156, Validation Accuracy: 0.4653, Loss: 1.4951\n",
      "Epoch   0 Batch 1450/2109 - Train Accuracy: 0.5430, Validation Accuracy: 0.4097, Loss: 1.2689\n",
      "Epoch   0 Batch 1460/2109 - Train Accuracy: 0.5241, Validation Accuracy: 0.4306, Loss: 1.4100\n",
      "Epoch   0 Batch 1470/2109 - Train Accuracy: 0.4987, Validation Accuracy: 0.4601, Loss: 1.3955\n",
      "Epoch   0 Batch 1480/2109 - Train Accuracy: 0.4576, Validation Accuracy: 0.4444, Loss: 1.7118\n",
      "Epoch   0 Batch 1490/2109 - Train Accuracy: 0.5696, Validation Accuracy: 0.4948, Loss: 1.3970\n",
      "Epoch   0 Batch 1500/2109 - Train Accuracy: 0.5177, Validation Accuracy: 0.4097, Loss: 1.2951\n",
      "Epoch   0 Batch 1510/2109 - Train Accuracy: 0.5013, Validation Accuracy: 0.4792, Loss: 1.5279\n",
      "Epoch   0 Batch 1520/2109 - Train Accuracy: 0.4193, Validation Accuracy: 0.5260, Loss: 2.0207\n",
      "Epoch   0 Batch 1530/2109 - Train Accuracy: 0.4859, Validation Accuracy: 0.4757, Loss: 1.3926\n",
      "Epoch   0 Batch 1540/2109 - Train Accuracy: 0.4861, Validation Accuracy: 0.4253, Loss: 1.3692\n",
      "Epoch   0 Batch 1550/2109 - Train Accuracy: 0.4385, Validation Accuracy: 0.4497, Loss: 1.6182\n",
      "Epoch   0 Batch 1560/2109 - Train Accuracy: 0.5168, Validation Accuracy: 0.4062, Loss: 1.3509\n",
      "Epoch   0 Batch 1570/2109 - Train Accuracy: 0.4471, Validation Accuracy: 0.4323, Loss: 1.4432\n",
      "Epoch   0 Batch 1580/2109 - Train Accuracy: 0.5397, Validation Accuracy: 0.4688, Loss: 1.4818\n",
      "Epoch   0 Batch 1590/2109 - Train Accuracy: 0.5770, Validation Accuracy: 0.4444, Loss: 1.2781\n",
      "Epoch   0 Batch 1600/2109 - Train Accuracy: 0.5328, Validation Accuracy: 0.4740, Loss: 1.3123\n",
      "Epoch   0 Batch 1610/2109 - Train Accuracy: 0.5344, Validation Accuracy: 0.4444, Loss: 1.4077\n",
      "Epoch   0 Batch 1620/2109 - Train Accuracy: 0.4518, Validation Accuracy: 0.4410, Loss: 1.5464\n",
      "Epoch   0 Batch 1630/2109 - Train Accuracy: 0.5399, Validation Accuracy: 0.4462, Loss: 1.3291\n",
      "Epoch   0 Batch 1640/2109 - Train Accuracy: 0.5312, Validation Accuracy: 0.4479, Loss: 1.5440\n",
      "Epoch   0 Batch 1650/2109 - Train Accuracy: 0.4609, Validation Accuracy: 0.4479, Loss: 1.5067\n",
      "Epoch   0 Batch 1660/2109 - Train Accuracy: 0.6810, Validation Accuracy: 0.4792, Loss: 1.1194\n",
      "Epoch   0 Batch 1670/2109 - Train Accuracy: 0.3281, Validation Accuracy: 0.4601, Loss: 2.1292\n",
      "Epoch   0 Batch 1680/2109 - Train Accuracy: 0.3821, Validation Accuracy: 0.4878, Loss: 1.6617\n",
      "Epoch   0 Batch 1690/2109 - Train Accuracy: 0.5769, Validation Accuracy: 0.4583, Loss: 1.2396\n",
      "Epoch   0 Batch 1700/2109 - Train Accuracy: 0.5791, Validation Accuracy: 0.4705, Loss: 1.2854\n",
      "Epoch   0 Batch 1710/2109 - Train Accuracy: 0.4328, Validation Accuracy: 0.4757, Loss: 1.6466\n",
      "Epoch   0 Batch 1720/2109 - Train Accuracy: 0.5298, Validation Accuracy: 0.4549, Loss: 1.3049\n",
      "Epoch   0 Batch 1730/2109 - Train Accuracy: 0.3797, Validation Accuracy: 0.4740, Loss: 1.5316\n",
      "Epoch   0 Batch 1740/2109 - Train Accuracy: 0.4787, Validation Accuracy: 0.4757, Loss: 1.3247\n",
      "Epoch   0 Batch 1750/2109 - Train Accuracy: 0.4740, Validation Accuracy: 0.4549, Loss: 1.3522\n",
      "Epoch   0 Batch 1760/2109 - Train Accuracy: 0.5359, Validation Accuracy: 0.4931, Loss: 1.5278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 1770/2109 - Train Accuracy: 0.6322, Validation Accuracy: 0.4653, Loss: 1.1603\n",
      "Epoch   0 Batch 1780/2109 - Train Accuracy: 0.4744, Validation Accuracy: 0.4531, Loss: 1.3505\n",
      "Epoch   0 Batch 1790/2109 - Train Accuracy: 0.4574, Validation Accuracy: 0.5069, Loss: 1.5471\n",
      "Epoch   0 Batch 1800/2109 - Train Accuracy: 0.5060, Validation Accuracy: 0.4878, Loss: 1.2918\n",
      "Epoch   0 Batch 1810/2109 - Train Accuracy: 0.6393, Validation Accuracy: 0.4757, Loss: 0.9467\n",
      "Epoch   0 Batch 1820/2109 - Train Accuracy: 0.6060, Validation Accuracy: 0.4878, Loss: 1.2049\n",
      "Epoch   0 Batch 1830/2109 - Train Accuracy: 0.4520, Validation Accuracy: 0.4531, Loss: 1.8639\n",
      "Epoch   0 Batch 1840/2109 - Train Accuracy: 0.5289, Validation Accuracy: 0.4913, Loss: 1.4958\n",
      "Epoch   0 Batch 1850/2109 - Train Accuracy: 0.4531, Validation Accuracy: 0.5035, Loss: 1.4205\n",
      "Epoch   0 Batch 1860/2109 - Train Accuracy: 0.5221, Validation Accuracy: 0.4618, Loss: 1.4742\n",
      "Epoch   0 Batch 1870/2109 - Train Accuracy: 0.5000, Validation Accuracy: 0.4635, Loss: 1.3216\n",
      "Epoch   0 Batch 1880/2109 - Train Accuracy: 0.5909, Validation Accuracy: 0.4531, Loss: 1.2668\n",
      "Epoch   0 Batch 1890/2109 - Train Accuracy: 0.5344, Validation Accuracy: 0.4705, Loss: 1.4076\n",
      "Epoch   0 Batch 1900/2109 - Train Accuracy: 0.5398, Validation Accuracy: 0.4566, Loss: 1.0890\n",
      "Epoch   0 Batch 1910/2109 - Train Accuracy: 0.5984, Validation Accuracy: 0.4670, Loss: 1.2172\n",
      "Epoch   0 Batch 1920/2109 - Train Accuracy: 0.4939, Validation Accuracy: 0.4601, Loss: 1.3004\n",
      "Epoch   0 Batch 1930/2109 - Train Accuracy: 0.6236, Validation Accuracy: 0.5226, Loss: 1.1792\n",
      "Epoch   0 Batch 1940/2109 - Train Accuracy: 0.4858, Validation Accuracy: 0.5017, Loss: 1.3561\n",
      "Epoch   0 Batch 1950/2109 - Train Accuracy: 0.3978, Validation Accuracy: 0.4670, Loss: 1.9957\n",
      "Epoch   0 Batch 1960/2109 - Train Accuracy: 0.4406, Validation Accuracy: 0.4601, Loss: 1.6240\n",
      "Epoch   0 Batch 1970/2109 - Train Accuracy: 0.3609, Validation Accuracy: 0.4722, Loss: 1.8308\n",
      "Epoch   0 Batch 1980/2109 - Train Accuracy: 0.2578, Validation Accuracy: 0.4878, Loss: 1.8420\n",
      "Epoch   0 Batch 1990/2109 - Train Accuracy: 0.4987, Validation Accuracy: 0.5122, Loss: 1.2620\n",
      "Epoch   0 Batch 2000/2109 - Train Accuracy: 0.5469, Validation Accuracy: 0.4427, Loss: 1.3386\n",
      "Epoch   0 Batch 2010/2109 - Train Accuracy: 0.5281, Validation Accuracy: 0.4635, Loss: 1.2417\n",
      "Epoch   0 Batch 2020/2109 - Train Accuracy: 0.5188, Validation Accuracy: 0.4462, Loss: 1.4231\n",
      "Epoch   0 Batch 2030/2109 - Train Accuracy: 0.4913, Validation Accuracy: 0.4462, Loss: 1.4705\n",
      "Epoch   0 Batch 2040/2109 - Train Accuracy: 0.6430, Validation Accuracy: 0.3906, Loss: 1.1670\n",
      "Epoch   0 Batch 2050/2109 - Train Accuracy: 0.4453, Validation Accuracy: 0.4306, Loss: 1.5898\n",
      "Epoch   0 Batch 2060/2109 - Train Accuracy: 0.5797, Validation Accuracy: 0.4132, Loss: 1.1651\n",
      "Epoch   0 Batch 2070/2109 - Train Accuracy: 0.5482, Validation Accuracy: 0.4306, Loss: 1.3837\n",
      "Epoch   0 Batch 2080/2109 - Train Accuracy: 0.6375, Validation Accuracy: 0.4653, Loss: 1.0121\n",
      "Epoch   0 Batch 2090/2109 - Train Accuracy: 0.4219, Validation Accuracy: 0.4774, Loss: 1.6057\n",
      "Epoch   0 Batch 2100/2109 - Train Accuracy: 0.6846, Validation Accuracy: 0.4601, Loss: 0.8397\n",
      "Epoch   1 Batch   10/2109 - Train Accuracy: 0.5276, Validation Accuracy: 0.4184, Loss: 1.3068\n",
      "Epoch   1 Batch   20/2109 - Train Accuracy: 0.3607, Validation Accuracy: 0.4219, Loss: 1.7246\n",
      "Epoch   1 Batch   30/2109 - Train Accuracy: 0.4965, Validation Accuracy: 0.4722, Loss: 1.2728\n",
      "Epoch   1 Batch   40/2109 - Train Accuracy: 0.4844, Validation Accuracy: 0.5729, Loss: 1.3853\n",
      "Epoch   1 Batch   50/2109 - Train Accuracy: 0.6312, Validation Accuracy: 0.5521, Loss: 1.2014\n",
      "Epoch   1 Batch   60/2109 - Train Accuracy: 0.4323, Validation Accuracy: 0.5469, Loss: 1.4278\n",
      "Epoch   1 Batch   70/2109 - Train Accuracy: 0.5107, Validation Accuracy: 0.5278, Loss: 1.4004\n",
      "Epoch   1 Batch   80/2109 - Train Accuracy: 0.3797, Validation Accuracy: 0.5365, Loss: 1.5957\n",
      "Epoch   1 Batch   90/2109 - Train Accuracy: 0.4016, Validation Accuracy: 0.5417, Loss: 1.6555\n",
      "Epoch   1 Batch  100/2109 - Train Accuracy: 0.5865, Validation Accuracy: 0.5260, Loss: 1.2009\n",
      "Epoch   1 Batch  110/2109 - Train Accuracy: 0.3542, Validation Accuracy: 0.5538, Loss: 1.8197\n",
      "Epoch   1 Batch  120/2109 - Train Accuracy: 0.5143, Validation Accuracy: 0.5590, Loss: 1.3433\n",
      "Epoch   1 Batch  130/2109 - Train Accuracy: 0.4050, Validation Accuracy: 0.4549, Loss: 1.3160\n",
      "Epoch   1 Batch  140/2109 - Train Accuracy: 0.5568, Validation Accuracy: 0.4566, Loss: 1.2535\n",
      "Epoch   1 Batch  150/2109 - Train Accuracy: 0.6120, Validation Accuracy: 0.4497, Loss: 1.1993\n",
      "Epoch   1 Batch  160/2109 - Train Accuracy: 0.5109, Validation Accuracy: 0.4340, Loss: 1.2950\n",
      "Epoch   1 Batch  170/2109 - Train Accuracy: 0.5810, Validation Accuracy: 0.4792, Loss: 1.1889\n",
      "Epoch   1 Batch  180/2109 - Train Accuracy: 0.6674, Validation Accuracy: 0.4375, Loss: 0.8508\n",
      "Epoch   1 Batch  190/2109 - Train Accuracy: 0.4856, Validation Accuracy: 0.4375, Loss: 1.2263\n",
      "Epoch   1 Batch  200/2109 - Train Accuracy: 0.5188, Validation Accuracy: 0.4462, Loss: 1.2023\n",
      "Epoch   1 Batch  210/2109 - Train Accuracy: 0.5328, Validation Accuracy: 0.4774, Loss: 1.3101\n",
      "Epoch   1 Batch  220/2109 - Train Accuracy: 0.6337, Validation Accuracy: 0.4514, Loss: 1.0919\n",
      "Epoch   1 Batch  230/2109 - Train Accuracy: 0.6536, Validation Accuracy: 0.5122, Loss: 1.1403\n",
      "Epoch   1 Batch  240/2109 - Train Accuracy: 0.6534, Validation Accuracy: 0.5330, Loss: 0.9534\n",
      "Epoch   1 Batch  250/2109 - Train Accuracy: 0.4844, Validation Accuracy: 0.4948, Loss: 1.3764\n",
      "Epoch   1 Batch  260/2109 - Train Accuracy: 0.4484, Validation Accuracy: 0.5208, Loss: 1.0727\n",
      "Epoch   1 Batch  270/2109 - Train Accuracy: 0.4857, Validation Accuracy: 0.4670, Loss: 1.5174\n",
      "Epoch   1 Batch  280/2109 - Train Accuracy: 0.6549, Validation Accuracy: 0.4462, Loss: 1.1318\n",
      "Epoch   1 Batch  290/2109 - Train Accuracy: 0.4938, Validation Accuracy: 0.4392, Loss: 1.3126\n",
      "Epoch   1 Batch  300/2109 - Train Accuracy: 0.5424, Validation Accuracy: 0.4722, Loss: 1.3499\n",
      "Epoch   1 Batch  310/2109 - Train Accuracy: 0.5228, Validation Accuracy: 0.4566, Loss: 1.3773\n",
      "Epoch   1 Batch  320/2109 - Train Accuracy: 0.5668, Validation Accuracy: 0.4635, Loss: 1.1530\n",
      "Epoch   1 Batch  330/2109 - Train Accuracy: 0.6649, Validation Accuracy: 0.4670, Loss: 0.9241\n",
      "Epoch   1 Batch  340/2109 - Train Accuracy: 0.4615, Validation Accuracy: 0.4826, Loss: 1.3314\n",
      "Epoch   1 Batch  350/2109 - Train Accuracy: 0.5344, Validation Accuracy: 0.4844, Loss: 1.4686\n",
      "Epoch   1 Batch  360/2109 - Train Accuracy: 0.5346, Validation Accuracy: 0.5017, Loss: 1.1017\n",
      "Epoch   1 Batch  370/2109 - Train Accuracy: 0.4899, Validation Accuracy: 0.4844, Loss: 1.2211\n",
      "Epoch   1 Batch  380/2109 - Train Accuracy: 0.5083, Validation Accuracy: 0.4271, Loss: 1.4087\n",
      "Epoch   1 Batch  390/2109 - Train Accuracy: 0.5908, Validation Accuracy: 0.3993, Loss: 1.0078\n",
      "Epoch   1 Batch  400/2109 - Train Accuracy: 0.5692, Validation Accuracy: 0.4340, Loss: 1.0552\n",
      "Epoch   1 Batch  410/2109 - Train Accuracy: 0.5851, Validation Accuracy: 0.4358, Loss: 1.0991\n",
      "Epoch   1 Batch  420/2109 - Train Accuracy: 0.6302, Validation Accuracy: 0.4948, Loss: 1.0488\n",
      "Epoch   1 Batch  430/2109 - Train Accuracy: 0.5104, Validation Accuracy: 0.4774, Loss: 1.2666\n",
      "Epoch   1 Batch  440/2109 - Train Accuracy: 0.4645, Validation Accuracy: 0.5087, Loss: 1.4774\n",
      "Epoch   1 Batch  450/2109 - Train Accuracy: 0.5128, Validation Accuracy: 0.5208, Loss: 1.1360\n",
      "Epoch   1 Batch  460/2109 - Train Accuracy: 0.6931, Validation Accuracy: 0.4826, Loss: 0.7329\n",
      "Epoch   1 Batch  470/2109 - Train Accuracy: 0.5170, Validation Accuracy: 0.4444, Loss: 1.2867\n",
      "Epoch   1 Batch  480/2109 - Train Accuracy: 0.4555, Validation Accuracy: 0.4740, Loss: 1.1803\n",
      "Epoch   1 Batch  490/2109 - Train Accuracy: 0.5594, Validation Accuracy: 0.4566, Loss: 1.0923\n",
      "Epoch   1 Batch  500/2109 - Train Accuracy: 0.5690, Validation Accuracy: 0.5208, Loss: 1.3751\n",
      "Epoch   1 Batch  510/2109 - Train Accuracy: 0.5404, Validation Accuracy: 0.4618, Loss: 1.1992\n",
      "Epoch   1 Batch  520/2109 - Train Accuracy: 0.5427, Validation Accuracy: 0.4549, Loss: 1.0382\n",
      "Epoch   1 Batch  530/2109 - Train Accuracy: 0.6203, Validation Accuracy: 0.4878, Loss: 1.0002\n",
      "Epoch   1 Batch  540/2109 - Train Accuracy: 0.6705, Validation Accuracy: 0.4931, Loss: 0.9350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch  550/2109 - Train Accuracy: 0.5094, Validation Accuracy: 0.5278, Loss: 1.1490\n",
      "Epoch   1 Batch  560/2109 - Train Accuracy: 0.3108, Validation Accuracy: 0.4965, Loss: 2.2779\n",
      "Epoch   1 Batch  570/2109 - Train Accuracy: 0.5996, Validation Accuracy: 0.5260, Loss: 0.9552\n",
      "Epoch   1 Batch  580/2109 - Train Accuracy: 0.5391, Validation Accuracy: 0.5799, Loss: 1.0302\n",
      "Epoch   1 Batch  590/2109 - Train Accuracy: 0.4976, Validation Accuracy: 0.5417, Loss: 1.2478\n",
      "Epoch   1 Batch  600/2109 - Train Accuracy: 0.5625, Validation Accuracy: 0.5017, Loss: 1.0908\n",
      "Epoch   1 Batch  610/2109 - Train Accuracy: 0.5521, Validation Accuracy: 0.5590, Loss: 1.0777\n",
      "Epoch   1 Batch  620/2109 - Train Accuracy: 0.5130, Validation Accuracy: 0.6094, Loss: 1.1929\n",
      "Epoch   1 Batch  630/2109 - Train Accuracy: 0.4583, Validation Accuracy: 0.5122, Loss: 1.0754\n",
      "Epoch   1 Batch  640/2109 - Train Accuracy: 0.4531, Validation Accuracy: 0.5469, Loss: 1.3328\n",
      "Epoch   1 Batch  650/2109 - Train Accuracy: 0.5938, Validation Accuracy: 0.5382, Loss: 1.0982\n",
      "Epoch   1 Batch  660/2109 - Train Accuracy: 0.6266, Validation Accuracy: 0.5052, Loss: 1.0106\n",
      "Epoch   1 Batch  670/2109 - Train Accuracy: 0.5859, Validation Accuracy: 0.5069, Loss: 0.9909\n",
      "Epoch   1 Batch  680/2109 - Train Accuracy: 0.5841, Validation Accuracy: 0.5469, Loss: 1.1270\n",
      "Epoch   1 Batch  690/2109 - Train Accuracy: 0.5144, Validation Accuracy: 0.5295, Loss: 1.0058\n",
      "Epoch   1 Batch  700/2109 - Train Accuracy: 0.5980, Validation Accuracy: 0.5503, Loss: 1.1274\n",
      "Epoch   1 Batch  710/2109 - Train Accuracy: 0.4659, Validation Accuracy: 0.5330, Loss: 1.3706\n",
      "Epoch   1 Batch  720/2109 - Train Accuracy: 0.4453, Validation Accuracy: 0.5139, Loss: 1.4986\n",
      "Epoch   1 Batch  730/2109 - Train Accuracy: 0.4785, Validation Accuracy: 0.5226, Loss: 1.3600\n",
      "Epoch   1 Batch  740/2109 - Train Accuracy: 0.4440, Validation Accuracy: 0.4427, Loss: 1.4629\n",
      "Epoch   1 Batch  750/2109 - Train Accuracy: 0.6042, Validation Accuracy: 0.4705, Loss: 0.9288\n",
      "Epoch   1 Batch  760/2109 - Train Accuracy: 0.5511, Validation Accuracy: 0.4896, Loss: 1.1546\n",
      "Epoch   1 Batch  770/2109 - Train Accuracy: 0.5453, Validation Accuracy: 0.4792, Loss: 0.8642\n",
      "Epoch   1 Batch  780/2109 - Train Accuracy: 0.7452, Validation Accuracy: 0.4792, Loss: 0.7323\n",
      "Epoch   1 Batch  790/2109 - Train Accuracy: 0.6859, Validation Accuracy: 0.5382, Loss: 0.8512\n",
      "Epoch   1 Batch  800/2109 - Train Accuracy: 0.6051, Validation Accuracy: 0.5330, Loss: 0.9561\n",
      "Epoch   1 Batch  810/2109 - Train Accuracy: 0.5203, Validation Accuracy: 0.5260, Loss: 1.4175\n",
      "Epoch   1 Batch  820/2109 - Train Accuracy: 0.7685, Validation Accuracy: 0.5104, Loss: 0.7903\n",
      "Epoch   1 Batch  830/2109 - Train Accuracy: 0.6891, Validation Accuracy: 0.4410, Loss: 0.7352\n",
      "Epoch   1 Batch  840/2109 - Train Accuracy: 0.6009, Validation Accuracy: 0.4358, Loss: 0.9759\n",
      "Epoch   1 Batch  850/2109 - Train Accuracy: 0.5516, Validation Accuracy: 0.4219, Loss: 1.0053\n",
      "Epoch   1 Batch  860/2109 - Train Accuracy: 0.6373, Validation Accuracy: 0.4271, Loss: 0.9776\n",
      "Epoch   1 Batch  870/2109 - Train Accuracy: 0.6062, Validation Accuracy: 0.4201, Loss: 0.9054\n",
      "Epoch   1 Batch  880/2109 - Train Accuracy: 0.5142, Validation Accuracy: 0.4219, Loss: 1.2713\n",
      "Epoch   1 Batch  890/2109 - Train Accuracy: 0.6781, Validation Accuracy: 0.4340, Loss: 0.9179\n",
      "Epoch   1 Batch  900/2109 - Train Accuracy: 0.5226, Validation Accuracy: 0.4062, Loss: 0.9272\n",
      "Epoch   1 Batch  910/2109 - Train Accuracy: 0.5284, Validation Accuracy: 0.4062, Loss: 1.5863\n",
      "Epoch   1 Batch  920/2109 - Train Accuracy: 0.4297, Validation Accuracy: 0.5000, Loss: 1.3968\n",
      "Epoch   1 Batch  930/2109 - Train Accuracy: 0.5458, Validation Accuracy: 0.4948, Loss: 1.1893\n",
      "Epoch   1 Batch  940/2109 - Train Accuracy: 0.5234, Validation Accuracy: 0.5382, Loss: 1.1318\n",
      "Epoch   1 Batch  950/2109 - Train Accuracy: 0.5385, Validation Accuracy: 0.5365, Loss: 0.9930\n",
      "Epoch   1 Batch  960/2109 - Train Accuracy: 0.5971, Validation Accuracy: 0.5799, Loss: 1.0726\n",
      "Epoch   1 Batch  970/2109 - Train Accuracy: 0.5130, Validation Accuracy: 0.5955, Loss: 0.9509\n",
      "Epoch   1 Batch  980/2109 - Train Accuracy: 0.6462, Validation Accuracy: 0.4757, Loss: 0.8381\n",
      "Epoch   1 Batch  990/2109 - Train Accuracy: 0.5469, Validation Accuracy: 0.4236, Loss: 1.1814\n",
      "Epoch   1 Batch 1000/2109 - Train Accuracy: 0.7273, Validation Accuracy: 0.4479, Loss: 0.7233\n",
      "Epoch   1 Batch 1010/2109 - Train Accuracy: 0.6435, Validation Accuracy: 0.4826, Loss: 0.9690\n",
      "Epoch   1 Batch 1020/2109 - Train Accuracy: 0.6024, Validation Accuracy: 0.5122, Loss: 0.9897\n",
      "Epoch   1 Batch 1030/2109 - Train Accuracy: 0.6222, Validation Accuracy: 0.5538, Loss: 0.8999\n",
      "Epoch   1 Batch 1040/2109 - Train Accuracy: 0.6974, Validation Accuracy: 0.4601, Loss: 0.8546\n",
      "Epoch   1 Batch 1050/2109 - Train Accuracy: 0.6264, Validation Accuracy: 0.4670, Loss: 0.9534\n",
      "Epoch   1 Batch 1060/2109 - Train Accuracy: 0.5256, Validation Accuracy: 0.5156, Loss: 0.9460\n",
      "Epoch   1 Batch 1070/2109 - Train Accuracy: 0.7203, Validation Accuracy: 0.4531, Loss: 0.7874\n",
      "Epoch   1 Batch 1080/2109 - Train Accuracy: 0.5868, Validation Accuracy: 0.4653, Loss: 1.0231\n",
      "Epoch   1 Batch 1090/2109 - Train Accuracy: 0.5828, Validation Accuracy: 0.4618, Loss: 1.0476\n",
      "Epoch   1 Batch 1100/2109 - Train Accuracy: 0.6146, Validation Accuracy: 0.4583, Loss: 0.9406\n",
      "Epoch   1 Batch 1110/2109 - Train Accuracy: 0.6615, Validation Accuracy: 0.4531, Loss: 0.7942\n",
      "Epoch   1 Batch 1120/2109 - Train Accuracy: 0.6022, Validation Accuracy: 0.4601, Loss: 1.0542\n",
      "Epoch   1 Batch 1130/2109 - Train Accuracy: 0.6406, Validation Accuracy: 0.4080, Loss: 0.9102\n",
      "Epoch   1 Batch 1140/2109 - Train Accuracy: 0.3919, Validation Accuracy: 0.4618, Loss: 1.5886\n",
      "Epoch   1 Batch 1150/2109 - Train Accuracy: 0.6055, Validation Accuracy: 0.4462, Loss: 1.0467\n",
      "Epoch   1 Batch 1160/2109 - Train Accuracy: 0.7328, Validation Accuracy: 0.4531, Loss: 0.7740\n",
      "Epoch   1 Batch 1170/2109 - Train Accuracy: 0.5578, Validation Accuracy: 0.4306, Loss: 0.9244\n",
      "Epoch   1 Batch 1180/2109 - Train Accuracy: 0.6775, Validation Accuracy: 0.4184, Loss: 0.8366\n",
      "Epoch   1 Batch 1190/2109 - Train Accuracy: 0.5241, Validation Accuracy: 0.4375, Loss: 1.1158\n",
      "Epoch   1 Batch 1200/2109 - Train Accuracy: 0.5384, Validation Accuracy: 0.4010, Loss: 0.8902\n",
      "Epoch   1 Batch 1210/2109 - Train Accuracy: 0.6133, Validation Accuracy: 0.4184, Loss: 0.8378\n",
      "Epoch   1 Batch 1220/2109 - Train Accuracy: 0.6339, Validation Accuracy: 0.4323, Loss: 0.9273\n",
      "Epoch   1 Batch 1230/2109 - Train Accuracy: 0.5664, Validation Accuracy: 0.4219, Loss: 1.0545\n",
      "Epoch   1 Batch 1240/2109 - Train Accuracy: 0.7045, Validation Accuracy: 0.4514, Loss: 0.6925\n",
      "Epoch   1 Batch 1250/2109 - Train Accuracy: 0.5359, Validation Accuracy: 0.4566, Loss: 1.2182\n",
      "Epoch   1 Batch 1260/2109 - Train Accuracy: 0.6250, Validation Accuracy: 0.4410, Loss: 0.9944\n",
      "Epoch   1 Batch 1270/2109 - Train Accuracy: 0.6733, Validation Accuracy: 0.4670, Loss: 0.8140\n",
      "Epoch   1 Batch 1280/2109 - Train Accuracy: 0.5406, Validation Accuracy: 0.4323, Loss: 1.1929\n",
      "Epoch   1 Batch 1290/2109 - Train Accuracy: 0.5734, Validation Accuracy: 0.4306, Loss: 1.0752\n",
      "Epoch   1 Batch 1300/2109 - Train Accuracy: 0.6875, Validation Accuracy: 0.4583, Loss: 0.7444\n",
      "Epoch   1 Batch 1310/2109 - Train Accuracy: 0.6159, Validation Accuracy: 0.4861, Loss: 0.9965\n",
      "Epoch   1 Batch 1320/2109 - Train Accuracy: 0.6151, Validation Accuracy: 0.4896, Loss: 0.8270\n",
      "Epoch   1 Batch 1330/2109 - Train Accuracy: 0.5755, Validation Accuracy: 0.4601, Loss: 1.0715\n",
      "Epoch   1 Batch 1340/2109 - Train Accuracy: 0.3663, Validation Accuracy: 0.4670, Loss: 1.5692\n",
      "Epoch   1 Batch 1350/2109 - Train Accuracy: 0.4391, Validation Accuracy: 0.4705, Loss: 1.4197\n",
      "Epoch   1 Batch 1360/2109 - Train Accuracy: 0.5346, Validation Accuracy: 0.6146, Loss: 0.9878\n",
      "Epoch   1 Batch 1370/2109 - Train Accuracy: 0.6752, Validation Accuracy: 0.6337, Loss: 0.8277\n",
      "Epoch   1 Batch 1380/2109 - Train Accuracy: 0.6317, Validation Accuracy: 0.6007, Loss: 1.1258\n",
      "Epoch   1 Batch 1390/2109 - Train Accuracy: 0.5518, Validation Accuracy: 0.6059, Loss: 1.0192\n",
      "Epoch   1 Batch 1400/2109 - Train Accuracy: 0.6990, Validation Accuracy: 0.5799, Loss: 0.6304\n",
      "Epoch   1 Batch 1410/2109 - Train Accuracy: 0.7031, Validation Accuracy: 0.4826, Loss: 0.7661\n",
      "Epoch   1 Batch 1420/2109 - Train Accuracy: 0.6615, Validation Accuracy: 0.4826, Loss: 0.8285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch 1430/2109 - Train Accuracy: 0.7088, Validation Accuracy: 0.4809, Loss: 0.8016\n",
      "Epoch   1 Batch 1440/2109 - Train Accuracy: 0.5984, Validation Accuracy: 0.5087, Loss: 0.9756\n",
      "Epoch   1 Batch 1450/2109 - Train Accuracy: 0.6680, Validation Accuracy: 0.5017, Loss: 0.7237\n",
      "Epoch   1 Batch 1460/2109 - Train Accuracy: 0.5938, Validation Accuracy: 0.5417, Loss: 0.8959\n",
      "Epoch   1 Batch 1470/2109 - Train Accuracy: 0.5534, Validation Accuracy: 0.5017, Loss: 0.7783\n",
      "Epoch   1 Batch 1480/2109 - Train Accuracy: 0.5469, Validation Accuracy: 0.4670, Loss: 1.0366\n",
      "Epoch   1 Batch 1490/2109 - Train Accuracy: 0.6562, Validation Accuracy: 0.5226, Loss: 0.7443\n",
      "Epoch   1 Batch 1500/2109 - Train Accuracy: 0.6156, Validation Accuracy: 0.5451, Loss: 0.7333\n",
      "Epoch   1 Batch 1510/2109 - Train Accuracy: 0.5990, Validation Accuracy: 0.4983, Loss: 0.9475\n",
      "Epoch   1 Batch 1520/2109 - Train Accuracy: 0.4232, Validation Accuracy: 0.4740, Loss: 1.4146\n",
      "Epoch   1 Batch 1530/2109 - Train Accuracy: 0.7031, Validation Accuracy: 0.5781, Loss: 0.8516\n",
      "Epoch   1 Batch 1540/2109 - Train Accuracy: 0.7344, Validation Accuracy: 0.5278, Loss: 0.7605\n",
      "Epoch   1 Batch 1550/2109 - Train Accuracy: 0.4938, Validation Accuracy: 0.5816, Loss: 1.0679\n",
      "Epoch   1 Batch 1560/2109 - Train Accuracy: 0.6538, Validation Accuracy: 0.5191, Loss: 0.7250\n",
      "Epoch   1 Batch 1570/2109 - Train Accuracy: 0.6178, Validation Accuracy: 0.5260, Loss: 0.8371\n",
      "Epoch   1 Batch 1580/2109 - Train Accuracy: 0.5913, Validation Accuracy: 0.5434, Loss: 0.8228\n",
      "Epoch   1 Batch 1590/2109 - Train Accuracy: 0.6228, Validation Accuracy: 0.5486, Loss: 0.7803\n",
      "Epoch   1 Batch 1600/2109 - Train Accuracy: 0.5953, Validation Accuracy: 0.6024, Loss: 0.7902\n",
      "Epoch   1 Batch 1610/2109 - Train Accuracy: 0.5687, Validation Accuracy: 0.5573, Loss: 0.9580\n",
      "Epoch   1 Batch 1620/2109 - Train Accuracy: 0.5599, Validation Accuracy: 0.5538, Loss: 0.9119\n",
      "Epoch   1 Batch 1630/2109 - Train Accuracy: 0.7309, Validation Accuracy: 0.5694, Loss: 0.7562\n",
      "Epoch   1 Batch 1640/2109 - Train Accuracy: 0.6886, Validation Accuracy: 0.5469, Loss: 0.7503\n",
      "Epoch   1 Batch 1650/2109 - Train Accuracy: 0.7591, Validation Accuracy: 0.4583, Loss: 0.6972\n",
      "Epoch   1 Batch 1660/2109 - Train Accuracy: 0.7578, Validation Accuracy: 0.5035, Loss: 0.6644\n",
      "Epoch   1 Batch 1670/2109 - Train Accuracy: 0.3681, Validation Accuracy: 0.5191, Loss: 1.8919\n",
      "Epoch   1 Batch 1680/2109 - Train Accuracy: 0.3764, Validation Accuracy: 0.5851, Loss: 1.0202\n",
      "Epoch   1 Batch 1690/2109 - Train Accuracy: 0.6466, Validation Accuracy: 0.4722, Loss: 0.7569\n",
      "Epoch   1 Batch 1700/2109 - Train Accuracy: 0.6660, Validation Accuracy: 0.4965, Loss: 0.7554\n",
      "Epoch   1 Batch 1710/2109 - Train Accuracy: 0.5938, Validation Accuracy: 0.4601, Loss: 1.0959\n",
      "Epoch   1 Batch 1720/2109 - Train Accuracy: 0.6491, Validation Accuracy: 0.4826, Loss: 0.7765\n",
      "Epoch   1 Batch 1730/2109 - Train Accuracy: 0.5859, Validation Accuracy: 0.5156, Loss: 0.9575\n",
      "Epoch   1 Batch 1740/2109 - Train Accuracy: 0.5241, Validation Accuracy: 0.5729, Loss: 0.8147\n",
      "Epoch   1 Batch 1750/2109 - Train Accuracy: 0.6910, Validation Accuracy: 0.5174, Loss: 0.7692\n",
      "Epoch   1 Batch 1760/2109 - Train Accuracy: 0.7312, Validation Accuracy: 0.5399, Loss: 0.7376\n",
      "Epoch   1 Batch 1770/2109 - Train Accuracy: 0.6178, Validation Accuracy: 0.5538, Loss: 0.7634\n",
      "Epoch   1 Batch 1780/2109 - Train Accuracy: 0.7031, Validation Accuracy: 0.5000, Loss: 0.7591\n",
      "Epoch   1 Batch 1790/2109 - Train Accuracy: 0.5099, Validation Accuracy: 0.4844, Loss: 1.1170\n",
      "Epoch   1 Batch 1800/2109 - Train Accuracy: 0.6791, Validation Accuracy: 0.4878, Loss: 0.6701\n",
      "Epoch   1 Batch 1810/2109 - Train Accuracy: 0.7995, Validation Accuracy: 0.5000, Loss: 0.4632\n",
      "Epoch   1 Batch 1820/2109 - Train Accuracy: 0.7199, Validation Accuracy: 0.5000, Loss: 0.6604\n",
      "Epoch   1 Batch 1830/2109 - Train Accuracy: 0.4643, Validation Accuracy: 0.5469, Loss: 1.3286\n",
      "Epoch   1 Batch 1840/2109 - Train Accuracy: 0.5477, Validation Accuracy: 0.5521, Loss: 1.2056\n",
      "Epoch   1 Batch 1850/2109 - Train Accuracy: 0.4219, Validation Accuracy: 0.5191, Loss: 0.8616\n",
      "Epoch   1 Batch 1860/2109 - Train Accuracy: 0.6862, Validation Accuracy: 0.5312, Loss: 0.7318\n",
      "Epoch   1 Batch 1870/2109 - Train Accuracy: 0.7188, Validation Accuracy: 0.5573, Loss: 0.6966\n",
      "Epoch   1 Batch 1880/2109 - Train Accuracy: 0.6648, Validation Accuracy: 0.5469, Loss: 0.6936\n",
      "Epoch   1 Batch 1890/2109 - Train Accuracy: 0.6328, Validation Accuracy: 0.5017, Loss: 0.9678\n",
      "Epoch   1 Batch 1900/2109 - Train Accuracy: 0.7372, Validation Accuracy: 0.6128, Loss: 0.6582\n",
      "Epoch   1 Batch 1910/2109 - Train Accuracy: 0.6922, Validation Accuracy: 0.5747, Loss: 0.8256\n",
      "Epoch   1 Batch 1920/2109 - Train Accuracy: 0.6458, Validation Accuracy: 0.5903, Loss: 0.8059\n",
      "Epoch   1 Batch 1930/2109 - Train Accuracy: 0.7116, Validation Accuracy: 0.5990, Loss: 0.5762\n",
      "Epoch   1 Batch 1940/2109 - Train Accuracy: 0.5668, Validation Accuracy: 0.6042, Loss: 0.7869\n",
      "Epoch   1 Batch 1950/2109 - Train Accuracy: 0.4099, Validation Accuracy: 0.5365, Loss: 1.2401\n",
      "Epoch   1 Batch 1960/2109 - Train Accuracy: 0.4521, Validation Accuracy: 0.5486, Loss: 1.1091\n",
      "Epoch   1 Batch 1970/2109 - Train Accuracy: 0.3906, Validation Accuracy: 0.5382, Loss: 1.1771\n",
      "Epoch   1 Batch 1980/2109 - Train Accuracy: 0.2799, Validation Accuracy: 0.5469, Loss: 1.0740\n",
      "Epoch   1 Batch 1990/2109 - Train Accuracy: 0.7083, Validation Accuracy: 0.4913, Loss: 0.7128\n",
      "Epoch   1 Batch 2000/2109 - Train Accuracy: 0.6732, Validation Accuracy: 0.4635, Loss: 0.8769\n",
      "Epoch   1 Batch 2010/2109 - Train Accuracy: 0.6516, Validation Accuracy: 0.4878, Loss: 0.7052\n",
      "Epoch   1 Batch 2020/2109 - Train Accuracy: 0.6562, Validation Accuracy: 0.5069, Loss: 0.7751\n",
      "Epoch   1 Batch 2030/2109 - Train Accuracy: 0.6319, Validation Accuracy: 0.4670, Loss: 0.9491\n",
      "Epoch   1 Batch 2040/2109 - Train Accuracy: 0.6983, Validation Accuracy: 0.4965, Loss: 0.6918\n",
      "Epoch   1 Batch 2050/2109 - Train Accuracy: 0.4703, Validation Accuracy: 0.4497, Loss: 0.9279\n",
      "Epoch   1 Batch 2060/2109 - Train Accuracy: 0.6422, Validation Accuracy: 0.4531, Loss: 0.6625\n",
      "Epoch   1 Batch 2070/2109 - Train Accuracy: 0.6523, Validation Accuracy: 0.4740, Loss: 0.8986\n",
      "Epoch   1 Batch 2080/2109 - Train Accuracy: 0.7797, Validation Accuracy: 0.4948, Loss: 0.5674\n",
      "Epoch   1 Batch 2090/2109 - Train Accuracy: 0.4878, Validation Accuracy: 0.4826, Loss: 1.0980\n",
      "Epoch   1 Batch 2100/2109 - Train Accuracy: 0.8213, Validation Accuracy: 0.4896, Loss: 0.4121\n",
      "Epoch   2 Batch   10/2109 - Train Accuracy: 0.5925, Validation Accuracy: 0.5243, Loss: 0.7821\n",
      "Epoch   2 Batch   20/2109 - Train Accuracy: 0.4727, Validation Accuracy: 0.6007, Loss: 1.1067\n",
      "Epoch   2 Batch   30/2109 - Train Accuracy: 0.6528, Validation Accuracy: 0.6441, Loss: 0.8507\n",
      "Epoch   2 Batch   40/2109 - Train Accuracy: 0.6146, Validation Accuracy: 0.7014, Loss: 0.8257\n",
      "Epoch   2 Batch   50/2109 - Train Accuracy: 0.6615, Validation Accuracy: 0.6788, Loss: 0.8183\n",
      "Epoch   2 Batch   60/2109 - Train Accuracy: 0.5938, Validation Accuracy: 0.6510, Loss: 0.8278\n",
      "Epoch   2 Batch   70/2109 - Train Accuracy: 0.5996, Validation Accuracy: 0.6111, Loss: 0.9459\n",
      "Epoch   2 Batch   80/2109 - Train Accuracy: 0.5406, Validation Accuracy: 0.6441, Loss: 0.9589\n",
      "Epoch   2 Batch   90/2109 - Train Accuracy: 0.4781, Validation Accuracy: 0.6562, Loss: 1.1975\n",
      "Epoch   2 Batch  100/2109 - Train Accuracy: 0.6779, Validation Accuracy: 0.6753, Loss: 0.7664\n",
      "Epoch   2 Batch  110/2109 - Train Accuracy: 0.4497, Validation Accuracy: 0.6944, Loss: 1.1313\n",
      "Epoch   2 Batch  120/2109 - Train Accuracy: 0.7161, Validation Accuracy: 0.6753, Loss: 0.6320\n",
      "Epoch   2 Batch  130/2109 - Train Accuracy: 0.7356, Validation Accuracy: 0.6250, Loss: 0.6852\n",
      "Epoch   2 Batch  140/2109 - Train Accuracy: 0.6591, Validation Accuracy: 0.6059, Loss: 0.8037\n",
      "Epoch   2 Batch  150/2109 - Train Accuracy: 0.6914, Validation Accuracy: 0.6007, Loss: 0.8297\n",
      "Epoch   2 Batch  160/2109 - Train Accuracy: 0.7172, Validation Accuracy: 0.5729, Loss: 0.7643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e9a71eb1c184>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m                  \u001b[0mtarget_sequence_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargets_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                  \u001b[0msource_sequence_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msources_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                  keep_prob: keep_probability})\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,valid_target,batch_size,source_vocab_to_int['<PAD>'],target_vocab_to_int['<PAD>']))\n",
    "\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     source_sequence_length: sources_lengths,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     source_sequence_length: valid_sources_lengths,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "pickle.dump(save_path, open('params.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "with open('preprocess.p', mode='rb') as in_file:\n",
    "    LOADED = pickle.load(in_file)\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = LOADED\n",
    "load_path = pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#XXXX MODIFIED FOR OUR PROBLEM XXXX\n",
    "def word_to_seq(word, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a word to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    '''Prepare the text for the model'''\n",
    "    seq=[]\n",
    "    for char in word:\n",
    "        if char not in vocab_to_int:\n",
    "            seq.append(vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            seq.append(vocab_to_int[char])\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_word(word):\n",
    "    word_seq = word_to_seq(word.lower(), source_vocab_to_int)\n",
    "\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "        loader.restore(sess, load_path)\n",
    "\n",
    "        input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "        logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "        target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "        source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "        keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        translate_logits = sess.run(logits, {input_data: [word_seq]*batch_size,\n",
    "                                             target_sequence_length: [len(word_seq)*2]*batch_size,\n",
    "                                             source_sequence_length: [len(word_seq)]*batch_size,\n",
    "                                             keep_prob: 1.0})[0]\n",
    "\n",
    "\n",
    "    return  '{}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence):\n",
    "    sentence_seq = \"\"\n",
    "    sentence = sentence.split(\" \")\n",
    "    for word in sentence:\n",
    "        sentence_seq += translate_word(word)[:-5]\n",
    "    return sentence_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "K W EH1 S T R AH0 S \n"
     ]
    }
   ],
   "source": [
    "print(translate_sentence(\"questross\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [13, 15, 6, 3, 10, 10, 9, 6, 5]\n",
      "  English Word: ['i', 'n', 'c', 'o', 'r', 'r', 'e', 'c', 't']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [10, 56, 53, 15, 67, 68, 70, 85]\n",
      "  Phones: AH0 N K AO1 R S T <EOS>\n"
     ]
    }
   ],
   "source": [
    "translate_sentence = 'incorrect'\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         source_sequence_length: [len(translate_sentence)]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Word: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  Phones: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
